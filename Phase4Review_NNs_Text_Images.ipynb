{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sasF_4ZyThMC"
   },
   "source": [
    "# Neural Networks Review and Practice Session!\n",
    "\n",
    "Using both Text and Image data with Neural Networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "erBQoXNEThME"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *OPEN IN GOOGLE COLAB IF I WANT TO RUN NEURAL NETWORKS!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lindsey imports this way so she doesn't have to worry about importing every function, methods, classes\n",
    "# from tensorflow import keras \n",
    "# from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hBCqdlsnThMF"
   },
   "outputs": [],
   "source": [
    "# Defining a results visualization function\n",
    "def visualize_training_results(history):\n",
    "    '''\n",
    "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    \n",
    "    Input: keras history object (output from trained model)\n",
    "    '''\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig.suptitle('Model Results')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE8C9eR9ThMG"
   },
   "source": [
    "## Part 0: Some Resources/References/Cheat Codes for Tuning Neural Networks\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/st83jeYy9L6Bq/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjn6ey8yThMG"
   },
   "source": [
    "### a. Adding nodes and layers\n",
    "\n",
    "- Number of hidden layers\n",
    "\n",
    "> For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification without convolutional layers, will need dozens of layers.\n",
    "\n",
    "- Number of neurons per layer\n",
    "\n",
    "> The number of neurons for the input and output layers is dependent on your data and the task. i.e. input dimensions are determined by your number or columns and your output layer for classification has just one node with a sigmoid activation function. For hidden layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.\n",
    "\n",
    "\n",
    "In general, you will get more bang for your buck by adding on more layers than adding more neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2ijW7YxThMH"
   },
   "source": [
    "### b. Activation functions\n",
    "\n",
    "* One thing important for activation functions is its **differentiability** because the derivative is used in the backpropagation process\n",
    "\n",
    "<img src='images/activation.png' width=500/>\n",
    "\n",
    "#### Activation functions for output layers (for supervised learning problems)\n",
    "\n",
    "1. For binary classification problems: sigmoid activation to coerce values between 0-1\n",
    "2. For multiclass classification: softmax activation, as it produces a non-negative vector that sums to 1 (probabilities of your test point belonging to the different classes)\n",
    "3. For regression problems: linear, or relu activation (it is linear and unbounded!)\n",
    "\n",
    "#### Activation functions for hidden layers\n",
    "\n",
    "Relevant/useful blog posts (by different authors, despite the similar titles): \n",
    "- [Exploring Activation Functions for Neural Networks](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)\n",
    "    - Discusses why adding non-linear activation functions helps us solve non-linear problems, as well as why an understanding of the derivatives of these activation functions helps us tune NNs more effectively\n",
    "- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "    - Builds the intuitition behind activation functions (despite less-than-stellar grammar and punctuation throughout)\n",
    "\n",
    "Notes: \n",
    "* Sigmoids are not used often because it has a small maximum derivative value and thus propagates only a small amount of error each time, leading to slow \"learning\" \n",
    "* This small-derivative-slow-learning issue is known as a **vanishing gradient** problem\n",
    "* Tanh is mathematically quite similar to a sigmoid function, thus also has the vanishing gradient issue, but not as bad\n",
    "* ReLu generally works well because its gradient is always 1, as long as the input is positive (no vanishing gradients), and negative inputs going to 0 can make your network lighter (no weights/biases are being updated)\n",
    "\n",
    "Here are resources on both the [vanishing gradient](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/m) and [exploding gradient](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmT8JZ1bThMI"
   },
   "source": [
    "### c. Loss functions\n",
    "\n",
    "Loss functions are akin to cost functions we were trying to minimize in gradient descent (i.e. RMSE for linear regression, Gini/entropy for trees)\n",
    "\n",
    "1. For regression problems, keras has **mean_squared_error** or **mean_absolute_error** as a loss function, or **mean_squared_logarithmic_error** if your target has potential outliers\n",
    "2. For binary classification: **binary_crossentropy** \n",
    "3. For multiclass problems: **categorical_crossentropy**\n",
    "\n",
    "[This article summarizes the above, and more.](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiIqNnUoThMI"
   },
   "source": [
    "### d. Optimizers\n",
    "\n",
    "Overview Resource: \n",
    "- [A (Quick) Guide to Neural Network Optimizers with Applications in Keras](https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4) blog post - an overview of the options\n",
    "\n",
    "Summary:\n",
    "* Different optimizers are just different methods/paths that your neural network can take to find optimal values\n",
    "* Experimentally, Adam (derived from *adaptive moment estimation*) is a good one to use - [here's more about Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "#### Quick optimizer summary\n",
    "\n",
    "* **RMSProp**: maintains per-parameter learning rates adapted based on the average of recent weight updates (e.g. how quickly it is changing). This does well on non-stationary problems (e.g. noisy data)\n",
    "\n",
    "* **Adagrad**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems)\n",
    "\n",
    "* **Adam**: realizes the benefits of both AdaGrad and RMSProp. Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLH6PN07ThMJ"
   },
   "source": [
    "### e. Learning rate\n",
    "\n",
    "* The learning rate is something you can define when you compile your model with the optimizer\n",
    "* Optimizers usually change up the learning rates, so this is just the *initial* learning rate\n",
    "* If you set it too low, training will eventually converge, but it will do so slowly\n",
    "* If you set it too high, it might actually diverge\n",
    "* If you set it slightly too high, it will converge at first but miss the local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL2zmOorThMJ"
   },
   "source": [
    "### f. Regularization\n",
    "\n",
    "* As a neural network learns, neuron weights settle into their context within the network\n",
    "* Weights of neurons are tuned for specific features providing some specialization\n",
    "* Neighboring neurons become too reliant on this specialization, which if taken too far can result in a fragile model too specialized to the training data\n",
    "* This reliance on context for a neuron during training is referred to as *complex co-adaptations*\n",
    "\n",
    "#### Methods\n",
    "1. You can add L1 or L2 regularization within each hidden layer\n",
    "2. You can also add a **dropout layer** \n",
    "3. Not technically *regularization*, but you can introduce **early stopping** so your model doesn't overtrain\n",
    "\n",
    "#### Dropout\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. You can add **dropout layers** in your neural network.\n",
    "\n",
    "<img src='images/thanos.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmT-7k3JThMK"
   },
   "source": [
    "# Part 1: Text Classification\n",
    "\n",
    "## Pre-Trained Networks and Word Embeddings and LSTMs, oh my!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b14VPTnEkQXY"
   },
   "source": [
    "*Kaggle Reference for Text Classification NN: https://www.kaggle.com/watts2/glove6b50dtxt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0XMI2GNUThMK"
   },
   "outputs": [],
   "source": [
    "# More specific imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j3SVQIXThMK"
   },
   "source": [
    "Data is: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "9REPQ0UNThMK",
    "outputId": "4d8b826e-2247-43cf-a1b7-d6a4ee3256c7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/IMDB_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WxRljW-qThML"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sXoMG8hqThML",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y70FzLdnThML"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our target value\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX2GQg9oThML"
   },
   "source": [
    "### Pre-Split Preprocessing\n",
    "\n",
    "Doing some initial preprocessing that can be done before the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OaPdwzSXThMM",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THIS IS NOT A CHILDREN\\'S MOVIE!!!<br /><br />This movie is like a \"bad acid trip\" for kids under the age of 5. For a month my 4 year old from time-to-time would ask me \"Why was that rabbit bleeding from its mouth\" or \"Why did the bulldozer bury all the rabbits?\". (And that wasn\\'t the worst of it). We stopped it about a 1/2 hour in but the damage had been done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten about the whole wretched thing. Whomever decided this movie should be marketed to children should be brought up on charges. ... (Go ahead censure me, my conscience is clear.).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out an example review...\n",
    "index_num = 15903 # Defining the index number of the review to explore\n",
    "\n",
    "df['review'].iloc[index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3U8DXLfThMM"
   },
   "source": [
    "We have some HTML tags inside these texts... will want to remove them. But how?\n",
    "\n",
    "Enter: Regular Expressions (regex).\n",
    "\n",
    "Testing: https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2szdwXFrThMM"
   },
   "outputs": [],
   "source": [
    "# Find the pattern to remove html tags\n",
    "import re\n",
    "\n",
    "html_tag_pattern = re.compile(r'<[^>]*>')\n",
    "\n",
    "test = html_tag_pattern.sub('', df['review'].iloc[index_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dj44qdIzThMM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THIS IS NOT A CHILDREN\\'S MOVIE!!!This movie is like a \"bad acid trip\" for kids under the age of 5. For a month my 4 year old from time-to-time would ask me \"Why was that rabbit bleeding from its mouth\" or \"Why did the bulldozer bury all the rabbits?\". (And that wasn\\'t the worst of it). We stopped it about a 1/2 hour in but the damage had been done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten about the whole wretched thing. Whomever decided this movie should be marketed to children should be brought up on charges. ... (Go ahead censure me, my conscience is clear.).'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w7IlVshNThMM"
   },
   "outputs": [],
   "source": [
    "# Apply our pattern to the dataset\n",
    "df['review'] = df['review'].map(lambda x: re.sub(r'<[^>]*>', '', x))\n",
    "\n",
    "# Same as\n",
    "# df['review'] = df['review'].map(lambda x: html_tag_pattern.sub('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f2RdH2x3ThMM",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THIS IS NOT A CHILDREN\\'S MOVIE!!!This movie is like a \"bad acid trip\" for kids under the age of 5. For a month my 4 year old from time-to-time would ask me \"Why was that rabbit bleeding from its mouth\" or \"Why did the bulldozer bury all the rabbits?\". (And that wasn\\'t the worst of it). We stopped it about a 1/2 hour in but the damage had been done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten about the whole wretched thing. Whomever decided this movie should be marketed to children should be brought up on charges. ... (Go ahead censure me, my conscience is clear.).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "df['review'].iloc[index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYpVS5M2ThMM"
   },
   "source": [
    "Let's also remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IuPit2tdThMM"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_c3uSmUeThMN"
   },
   "outputs": [],
   "source": [
    "# Neat bit of code!\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word.lower() not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NZe97dkx4_J-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHILDREN\\'S MOVIE!!!This movie like \"bad acid trip\" kids age 5. month 4 year old time-to-time would ask \"Why rabbit bleeding mouth\" \"Why bulldozer bury rabbits?\". (And worst it). stopped 1/2 hour damage done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten whole wretched thing. Whomever decided movie marketed children brought charges. ... (Go ahead censure me, conscience clear.).'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "df['review'].iloc[index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdheiDINThMN"
   },
   "source": [
    "Can also pre-process our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oBU2xn4PThMN"
   },
   "outputs": [],
   "source": [
    "# Create a target map\n",
    "target_map = {'positive': 1,\n",
    "              'negative': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UBoDANwVThMN"
   },
   "outputs": [],
   "source": [
    "# Map it\n",
    "df['sentiment'] = df['sentiment'].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "p9sxH-mAThMN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production. filming technique...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's family little boy (Jake) thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love Time Money\" visually stu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One reviewers mentioned watching 1 Oz episode ...          1\n",
       "1  wonderful little production. filming technique...          1\n",
       "2  thought wonderful way spend time hot summer we...          1\n",
       "3  Basically there's family little boy (Jake) thi...          0\n",
       "4  Petter Mattei's \"Love Time Money\" visually stu...          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti4bhHANThMN"
   },
   "source": [
    "### Split, and then Post-Split Processing\n",
    "Now let's perform a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Kwth5eaLThMN"
   },
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WLCtO8mzThMN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hSz57L0aThMN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHILDREN\\'S MOVIE!!!This movie like \"bad acid trip\" kids age 5. month 4 year old time-to-time would ask \"Why rabbit bleeding mouth\" \"Why bulldozer bury rabbits?\". (And worst it). stopped 1/2 hour damage done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten whole wretched thing. Whomever decided movie marketed children brought charges. ... (Go ahead censure me, conscience clear.).'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to find that same review now that the index is shuffled\n",
    "train_index_num = X_train.index.get_loc(15903)\n",
    "X_train.iloc[train_index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00_Z_jvPThMN"
   },
   "source": [
    "### Vanilla Text Classification... What Would We Do?\n",
    "\n",
    "Aka what would this look like without a NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "30dv7TZ7ThMN"
   },
   "outputs": [],
   "source": [
    "# Let's use a TF-IDF vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EynxkRaBThMN"
   },
   "outputs": [],
   "source": [
    "# What parameters should we set? What steps have we already done, what do we still need to do?\n",
    "# Already removed stopwords!\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.95,  # removes words that appear in more than 95% of docs\n",
    "    min_df=2 # removes words that appear 2 or fewer times\n",
    "    #This is dimensionality reduction\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gIf3JwvQThMO"
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsdoa1mKThMO"
   },
   "source": [
    "#### Explore Our Vectorized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XwzxbmvBThMO",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHILDREN\\'S MOVIE!!!This movie like \"bad acid trip\" kids age 5. month 4 year old time-to-time would ask \"Why rabbit bleeding mouth\" \"Why bulldozer bury rabbits?\". (And worst it). stopped 1/2 hour damage done. Intensely morbid, oppressive, violent. Fortunately he\\'s finally forgotten whole wretched thing. Whomever decided movie marketed children brought charges. ... (Go ahead censure me, conscience clear.).'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at that second example again\n",
    "X_train.iloc[train_index_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fg8iOWI0ThMO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22171"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PqJtROlsThMO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15903    CHILDREN'S MOVIE!!!This movie like \"bad acid t...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.loc[X_train.str.contains('CHILDREN\\'S MOVIE!!!')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_Y8UrwL6ThMO"
   },
   "outputs": [],
   "source": [
    "# Creating a df of tf-idf values, where each column is a word in the vocabulary\n",
    "tfidf_train_df = pd.DataFrame(X_train_vec.toarray(), \n",
    "                              columns=vectorizer.get_feature_names(), \n",
    "                              index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AWyfbrrJThMO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "censure       0.248884\n",
       "bulldozer     0.228991\n",
       "why           0.217933\n",
       "whomever      0.202968\n",
       "children      0.202369\n",
       "rabbits       0.197874\n",
       "oppressive    0.188972\n",
       "bleeding      0.186441\n",
       "bury          0.185497\n",
       "charges       0.183285\n",
       "marketed      0.182048\n",
       "intensely     0.174200\n",
       "morbid        0.174200\n",
       "wretched      0.170670\n",
       "conscience    0.168263\n",
       "Name: 15903, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing that row once it's been vectorized\n",
    "test_doc = tfidf_train_df.iloc[train_index_num]\n",
    "\n",
    "test_doc[test_doc > 0].sort_values(ascending=False).head(15) # Showing values > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNQ8hdXaThMO"
   },
   "source": [
    "What does this tell you about the word \"censure\" in the this document?\n",
    "\n",
    "- Censure is rarer than bulldozer in other documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gb3PE4h2ThMO"
   },
   "outputs": [],
   "source": [
    "# Now let's model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5nhlWHx0ThMO",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86592"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "classifier.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kT7I3GBThMO"
   },
   "source": [
    "Evaluate:\n",
    "\n",
    "- Not bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XqySDByv5ao9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.50248\n",
       "1    0.49752\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline Understanding\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TO9442j5c2M"
   },
   "source": [
    "*We can consider our baseline model to be 86% accurate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqrYvTBfThMO"
   },
   "source": [
    "### Moving to NN-Based Text Classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKL-AivhThMO"
   },
   "source": [
    "#### Different Pre-Processing Steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HOjsfOzThMO"
   },
   "source": [
    "Let's walk through these steps first, then discuss why we didn't just use vectorized text.\n",
    "\n",
    "Going to use keras's tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2u1cp8oThMO"
   },
   "outputs": [],
   "source": [
    "# Find our longest review - will need for padding later\n",
    "max_length = max([len(s.split()) for s in X_train])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE8PX98uThMO"
   },
   "outputs": [],
   "source": [
    "# Now to tokenize\n",
    "# Recommend checking out their default values - they're removing punctuation for us!\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3ujMs6woAIL"
   },
   "outputs": [],
   "source": [
    "#Most common words not removed\n",
    "tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0fRQpGiThMP"
   },
   "outputs": [],
   "source": [
    "# What is this doing? \n",
    "#Let's look at the first 10 key-value pairs in the word_index dict\n",
    "\n",
    "list(tokenizer.word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBMX_IlKThMP"
   },
   "outputs": [],
   "source": [
    "# Same example, after processing\n",
    "print(X_train_token[train_index_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DiXHrvJThMP"
   },
   "outputs": [],
   "source": [
    "# Grab the corpus size\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J417WJMeThMP"
   },
   "outputs": [],
   "source": [
    "# Now, let's pad so each review is the same length as our longest review\n",
    "# Basically, adding zeros at the end\n",
    "\n",
    "X_train_processed = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_token, maxlen=max_length, padding='post')\n",
    "X_test_processed = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_token, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbE0JILIThMP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train_processed[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0sKLEFXThMP"
   },
   "source": [
    "#### Why Couldn't We Just Use TF-IDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lD8s0LvThMP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the vectorized text\n",
    "tfidf_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD_adx9aThMP"
   },
   "outputs": [],
   "source": [
    "# Look at the preprocessed text from keras\n",
    "X_train_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztTxyqfGThMP"
   },
   "source": [
    "What is the difference? Specifically, what are the columns representing in each of these? What are the numbers?\n",
    "\n",
    "- In `tfidf_train_df`, each column is a word in our Corpus\n",
    "- In `X_train_processed`, each column is the placement of the word in the sentence. It's value is the rank of the word in the dictionary. For example, every time `1` shows up, that means `movie`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqOz_OFqThMP"
   },
   "source": [
    "## Now: Using Pre-Trained Embeddings and NNs for NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ql7etz9ThMP"
   },
   "source": [
    "For the most part, following this example: https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n",
    "\n",
    "Also relevant: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjuA44HCThMP"
   },
   "source": [
    "### Pre-Trained Word Embeddings: GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "The link to download the GloVe files: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "> **You will need to download the GloVe embeddings directly, since these files are all too big for github!**\n",
    "\n",
    "The below function and code comes from: https://realpython.com/python-keras-text-classification/#using-pretrained-word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n_r0y7HqCFJ"
   },
   "source": [
    "*Lindsey didn't write the following code, but it works*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3reQ8JfThMP"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(glove_filepath, word_index, embedding_dim):\n",
    "    '''\n",
    "    Grabs the embeddings just for the words in our vocabulary\n",
    "    \n",
    "    Inputs:\n",
    "    glove_filepath - string, location of the glove text file to use\n",
    "    word_index - word_index attribute from the keras tokenizer\n",
    "    embedding_dim - int, number of dimensions to embed, a hyperparameter\n",
    "    \n",
    "    Output:\n",
    "    embedding_matrix - numpy array of embeddings\n",
    "    '''\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(glove_filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_70t_-ptThMP"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt',\n",
    "                                           tokenizer.word_index, \n",
    "                                           embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QckkGa0MThMP"
   },
   "outputs": [],
   "source": [
    "#Number of words in our vocabulary, by number of dimensions we are limiting it to\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IIWJMccThMP"
   },
   "source": [
    "How is this different from previous preprocessing steps?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLj7P63AThMQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time to model!\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=max_length, \n",
    "                           trainable=False)) # Note - not retraining the embedding layer\n",
    "model.add(layers.Flatten()) # flattening these layers down before connecting to dense layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txc2uNP6ThMQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM3kmQhTThMQ"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_processed, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMvBWMsIThMQ"
   },
   "source": [
    "Evaluate:\n",
    "\n",
    "- Super overfit and not better than our Naive Bayes. This tells us we don't need a NN for this problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVULx7vfThMQ"
   },
   "source": [
    "### Treat Embeddings as Starting Weights, but Allow Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO3S8CUlThMQ"
   },
   "outputs": [],
   "source": [
    "# Time to model!\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=max_length, \n",
    "                           trainable=True)) # Now it can retrain the embedding layer\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3GFA06zThMQ"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test))\n",
    "# Takes about... 3 minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF7p_lKWThMQ"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_processed, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BduxGU5BThMQ"
   },
   "source": [
    "Evalutate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws0SZObTThMQ"
   },
   "source": [
    "### Early Stopping\n",
    "\n",
    "Patience: how many epochs that model can keep running without improvement before the training is stopped\n",
    "\n",
    "Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J00dpaTFThMQ"
   },
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ahozb47sNr-"
   },
   "source": [
    "*If we don't know how many epochs to run, use EarlyStopping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QxiklYaThMQ"
   },
   "outputs": [],
   "source": [
    "# Combine with a model saving feature, so it saves as it improves\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jOBGiXbThMQ"
   },
   "outputs": [],
   "source": [
    "# Same model as just before this\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4BXJ5ZLThMQ"
   },
   "outputs": [],
   "source": [
    "# Just adding more epochs\n",
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test),\n",
    "                    callbacks=[es, mc])\n",
    "\n",
    "# This takes... a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_CjgVWeThMQ"
   },
   "source": [
    "### LSTM\n",
    "\n",
    "Note: there might still be a bug in tensorflow related to the newest numpy version, if you have numpy version 1.20+ this might not work.\n",
    "\n",
    "https://github.com/tensorflow/models/issues/9706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCBymwlcshWw"
   },
   "source": [
    "*Really good for Time-Series and figuring out what the next word should be*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9czVXnUThMQ"
   },
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxexwB_gThMR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=max_length,\n",
    "                           trainable=True))\n",
    "# Replacing our Flattening layer with an LSTM\n",
    "# Adding some dropout to prevent overfitting - note the two ways to do so\n",
    "model.add(layers.LSTM(embedding_dim, \n",
    "                      dropout=0.2,\n",
    "                      return_sequences=True))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLrOH8tpThMR"
   },
   "source": [
    "We could do this all here... or we could move over to Kaggle and run this with GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD7FFt9QThMR"
   },
   "source": [
    "### Saving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNOJE8qitkbh"
   },
   "source": [
    "*These steps are good for deploying your model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw9NufG2ThMR"
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqpLUexLThMR"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('model.h5')\n",
    "my_model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCBqJsUBThMR"
   },
   "outputs": [],
   "source": [
    "my_model.evaluate(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klEjecVgThMR"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Sklearn's [Working with Text Data Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "What else can we do with natural language data beyond text classification? \n",
    "\n",
    "- [This blog post](https://blog.aureusanalytics.com/blog/5-natural-language-processing-techniques-for-extracting-information) by Aureus Analytics provides an overview of other machine learning techniques used to extract meaning from text: Named Entity Recognition, Sentiment Analysis, Text Summarization, Aspect Mining and Topic Modeling\n",
    "\n",
    "### Neural Network Vectorizer Resources:\n",
    "\n",
    "Want another way to embed words for machine learning? Check out Word2Vec - a way of vectorizing text that tries to capture the relationships between words. See the image below, from [this paper](https://arxiv.org/pdf/1310.4546.pdf) from Google developers, that introduced a Skip-gram neural network model that's been utilized by Word2Vec (which is a tool you can use to implement this model). You'll note that the distance between each country and it's capital city is about the same - that distance actually has meaning, and thus you can imagine that the difference between `cat` and `kitten` would be the same as the difference between `dog` and `puppy`. Et cetera!\n",
    "\n",
    "![screenshot from a paper on the Skip-gram model from devleopers at Google, https://arxiv.org/pdf/1310.4546.pdf](images/Fig2-DsitributedRepresentationsOfWordsAndPhrasesAndTheirCompositionality.png)\n",
    "\n",
    "- [Pathmind's A.I. Wiki - A Beginner's Guide to Word2Vec](https://wiki.pathmind.com/word2vec)\n",
    "- [Chris McCormick's Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [What is the difference between Word2Vec and GloVe?](https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2S-1F9AThMR"
   },
   "source": [
    "-----\n",
    "\n",
    "# Part 2: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "VXr9NuT7ThMR",
    "outputId": "e507a30d-d54d-48ef-b6a6-e41536c67fc5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New dataset!\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "plt.show()\n",
    "# If you get a \"downloading data\" thing, worry not, shouldn't take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Mbmvn2MThMR",
    "outputId": "2c38120f-5b23-4dad-cd42-f33925568c5e"
   },
   "outputs": [],
   "source": [
    "# Checking the class balance of our data\n",
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRD8rZ1PXSmI"
   },
   "source": [
    "*Balanced classes (i.e. Airplane, automobile, bird, etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ByY_kUbThMR",
    "outputId": "c2935103-a0ba-46d7-b13c-d28e11258fc0"
   },
   "outputs": [],
   "source": [
    "# Still need to prep our data\n",
    "# Scale images to the [0, 1] range - max is 255\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (32, 32, 3)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cSVvh9cThMR"
   },
   "source": [
    "### So - What is our Input Shape? What is our Output Shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCppmCQ4ThMR"
   },
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3) # Size of each image - 32x32 for 3 layers\n",
    "output_shape = 10 # Number of classes, aka number of possible targets (multi class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUngXAqVZCYD",
    "outputId": "177d1d6c-e086-431f-d9e7-c5a26be137cb"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzo8AIJ8ZFXL"
   },
   "source": [
    "*If you left y_train like this, it would try and solve as a regression problem <br> So use `to_categorical` to OneHotEncode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aw2YbrrJThMR",
    "outputId": "55bfe693-900b-477e-e35f-7797c38abff1"
   },
   "outputs": [],
   "source": [
    "# We need to prep our outputs\n",
    "# Different from binary classification!\n",
    "y_train = keras.utils.to_categorical(y_train, output_shape)\n",
    "y_test = keras.utils.to_categorical(y_test, output_shape)\n",
    "\n",
    "print(\"y_train shape:\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJgqP8xFThMR",
    "outputId": "6f89a6c7-19a3-40a8-8f03-98d2f67cfc6e"
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp6tI5--ZX2l"
   },
   "source": [
    "*Notice how the first target is a 6 and now there is a 1 in the 6th column*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQsaUZx4ThMR"
   },
   "source": [
    "## First - Simple Multi-Layer Perceptron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wH0JmdlZrWY"
   },
   "source": [
    "*The following in instantiating the model as a list, unlike the `.add()` that we did on the NN checkpoint this morning<br> One thing about NN is that there are so many ways to do the same thing. Just be careful to ask yourself \"Do I need this person's code example\" or \"How can I repurpose this code example for my code\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtxHtztYThMR",
    "outputId": "010a3f09-64e7-4ef7-b157-0cd1b4288da0"
   },
   "outputs": [],
   "source": [
    "# Model building in a single list\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape), # Don't always need this input separately\n",
    "        layers.Flatten(), # need to flatten our images to be one long array\n",
    "        layers.Dense(64, activation=\"tanh\"),\n",
    "        layers.Dense(output_shape, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9YYqPvqaK4G"
   },
   "source": [
    "*We use a softmax because it's a multiclass target/problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zQW72yxThMS"
   },
   "outputs": [],
   "source": [
    "batch_size = int(X_train.shape[0]/20) #divide the number of rows in our X_train by 20 (just because)\n",
    "epochs = 15\n",
    "\n",
    "# Compiling our model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQwosF31bWB8",
    "outputId": "fa73e981-4298-41ec-94c8-3376ad67f27f"
   },
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeoVAO8zbgwg"
   },
   "source": [
    "*Lindsey got better results when she changed the batch size to `int(X_train.shape[0]/100)`<br> Therefore `batch_size = 500`<br> This will take longer but give your model more time to adjust the parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWz3Pf8_ThMS",
    "outputId": "0b1d51ee-eade-4afe-fd93-c8585a420b05"
   },
   "outputs": [],
   "source": [
    "# Fit our model (save output to a history variable)\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "SCAfVffbThMS",
    "outputId": "54911ba7-58c9-4dae-ceb1-7bcb50e2fa23"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ggIqm-AThMS"
   },
   "source": [
    "Evaluate:\n",
    "\n",
    "- Not Good\n",
    "- I could give it more epochs because the loss plot isn't trailing off yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApWhB69WThMS"
   },
   "source": [
    "## Building a CNN - AKA Incorporating Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avTVKJoEThMS"
   },
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation.\n",
    "\n",
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "2. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9mSCUgwThMS"
   },
   "source": [
    "### Pooling Layers\n",
    "\n",
    "After a convolutional layer, the feature maps are fed into a max pool layer. Like convolutions, this method is applied one patch at a time (usually 2x2). Max pooling simply takes the largest value from one patch of an image, places it in a new matrix next to the max values from other patches, and discards the rest of the information contained in the activation maps. Other methods exist such as average pooling (taking an average of the patch).\n",
    "\n",
    "<img src='images/maxpool.png'/>\n",
    "\n",
    "This process results in a new feature map with reduced dimensionality that is then passed into another convolution layer to continue the pattern finding process. These steps are repeated until they are passed to a fully connected layer that proceeds to classify the image using the identified patterns.\n",
    "\n",
    "### Flattening\n",
    "\n",
    "Once the neural network has collected a series of patterns that an image contains, it is ready to make a guess as to what the image is. In order to do so, it starts by **flattening** the 2D matrix into a 1D vector, so it can be passed into a normal densely connected layer for classification. Then using this vector, one or many densely connected layers will make a prediction as to what the image is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DxBoI6PThMS"
   },
   "source": [
    "Reference for this dataset/NN architecture: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0LpV037eBTt"
   },
   "source": [
    "*More often, it starts big (128) and goes smaller (32) <br> Andrew said \"the 32, 64, 128 is the number of filters. the output shapes are getting smaller like you said\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u20B_-NqThMS"
   },
   "outputs": [],
   "source": [
    "# Model building using \"add\"\n",
    "\n",
    "cnn = keras.Sequential()\n",
    "# We defined a variable input_shape earlier, can use that here\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# now, to get the proper output\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu'))\n",
    "cnn.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsnmWtwTThMS",
    "outputId": "1f721e53-66db-4baa-8931-73b3c470341f"
   },
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92FyY6XhThMS",
    "outputId": "4c4bc571-59e6-47a3-81f8-383709c3f3f2"
   },
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=2, # small since this is just a demo\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_test, y_test))\n",
    "# note - even with just 2 epochs this'll take at least a few minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTNaFUQdeZyQ"
   },
   "source": [
    "*If you don't understand what's going on with the model, Welcome to Blackbox Models. You will not be able to explain what's going on. That's ok becuase you don't want to use a NN for explainability. <br> If you really wanted to see what was happening, use LIME*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unMaRFMGfIWq"
   },
   "source": [
    "*You can also go to this github for reference: https://github.com/marcotcr/lime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "d33Ic8LLThMS",
    "outputId": "5fbf4309-9fb8-43e0-8346-0b8c45e6394a"
   },
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONudaXiLThMS"
   },
   "source": [
    "A worked-through example on this dataset: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uycVZiP0ThMS"
   },
   "source": [
    "### Using Pre-Trained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EovSzEdPThMS"
   },
   "source": [
    "A pretrained network (also known as a convolutional base for CNNs) consists of layers that have already been trained on typically general data. For images, these layers have already learned general patterns, textures, colors, etc. such that when you feed in your training data, certain features can immediately be detected. This part is **feature extraction**.\n",
    "\n",
    "You typically add your own final layers to train the network to classify/regress based on your problem. This component is **fine tuning**\n",
    "\n",
    "Here are the pretrained image classification models that exist within Keras: https://keras.io/api/applications/\n",
    "\n",
    "VGG19: https://keras.io/api/applications/vgg/#vgg19-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xI85qqygJC5"
   },
   "source": [
    "*This is giving pre-trained weights at the start, giving it a better starting point*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqrW6X48ThMS"
   },
   "outputs": [],
   "source": [
    "#from keras.applications import VGG19 #Lindsey is using this because she can\n",
    "from keras.applications.vgg19 import VGG19 #Maybe this will work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtFutfxjThMS",
    "outputId": "f88c65ea-1652-47f0-aec7-094b74f67056"
   },
   "outputs": [],
   "source": [
    "pretrained = VGG19(weights='imagenet',\n",
    "                   include_top=False, # Allows us to set input shape\n",
    "                   input_shape=input_shape) \n",
    "# May download data at this step, shouldn't take long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0FVXUorgUAH"
   },
   "source": [
    "*If `include_top=True` you need to resize the pictures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2GdkppEThMS",
    "outputId": "ec639966-8c48-430f-9e3c-a96b4b6fdbd8"
   },
   "outputs": [],
   "source": [
    "pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dWMDvPvThMT"
   },
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential()\n",
    "cnn.add(pretrained)\n",
    "\n",
    "# freezing layers so they don't get re-trained with your new data\n",
    "for layer in cnn.layers:\n",
    "    layer.trainable=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN3Kh60AgcZI"
   },
   "source": [
    "*She is freezing it (`layer.trainable=False`) so that it doesn't change the weights for each layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUpaTtyAThMT"
   },
   "outputs": [],
   "source": [
    "# adding our own dense layers\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu'))\n",
    "cnn.add(layers.Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYnSXy33ThMT",
    "outputId": "cf54f649-ecb4-4809-f6a6-d15e6380233a"
   },
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFOhESsBThMT",
    "outputId": "762de4a5-acd3-406d-97a1-63e4a9ab62ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to verify that the weights are \"frozen\" \n",
    "for layer in cnn.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr1EvYVeThMT"
   },
   "source": [
    "With this you can now compile and fit your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH_KonHIThMT"
   },
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEPlAIPJThMT"
   },
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=5, \n",
    "                  batch_size=64,\n",
    "                  validation_data=(X_test_resized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDU6cAK8ThMT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "avTVKJoEThMS"
   ],
   "name": "Phase4Review-NNs-Text-Images.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
